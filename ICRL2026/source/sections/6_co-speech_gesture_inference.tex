
% \section{Default Notation}

% In an attempt to encourage standardized notation, we have included the
% notation file from the textbook, \textit{Deep Learning}
% \cite{goodfellow2016deep} available at
% \url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
% is not required and can be disabled by commenting out
% \texttt{math\_commands.tex}.


% \centerline{\bf Numbers and Arrays}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1in}p{3.25in}}
% $\displaystyle a$ & A scalar (integer or real)\\
% $\displaystyle \va$ & A vector\\
% $\displaystyle \mA$ & A matrix\\
% $\displaystyle \tA$ & A tensor\\
% $\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
% $\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
% $\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
% $\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
% $\displaystyle \ra$ & A scalar random variable\\
% $\displaystyle \rva$ & A vector-valued random variable\\
% $\displaystyle \rmA$ & A matrix-valued random variable\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Sets and Graphs}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \sA$ & A set\\
% $\displaystyle \R$ & The set of real numbers \\
% $\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
% $\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
% $\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
% $\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
% $\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
% $\displaystyle \gG$ & A graph\\
% $\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
% \end{tabular}
% \vspace{0.25cm}


% \centerline{\bf Indexing}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
% $\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
% $\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
% $\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
% $\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
% $\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
% $\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
% $\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}


% \centerline{\bf Calculus}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% % NOTE: the [2ex] on the next line adds extra height to that row of the table.
% % Without that command, the fraction on the first line is too tall and collides
% % with the fraction on the second line.
% $\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
% $\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
% $\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
% $\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
% $\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
% $\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
% $\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
% $\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
% $\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Probability and Information Theory}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
% $\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
% a variable whose type has not been specified\\
% $\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
% $\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
% $\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
% $\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
% $\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
% $\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
% $\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
% over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Functions}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
% $\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
%   $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
%   (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
% $\displaystyle \log x$ & Natural logarithm of $x$ \\
% $\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
% $\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
% $\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
% $\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
% $\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
% $\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}



\section{Co-speech Gesture Inference}
\label{sec:co-speech_gesture_inference}
When given a speech as input, our system segments it into normalized feature blocks $\{\vect{A}_i^{\eqword{low}}, \vect{A}_i^{\eqword{high}}, \vect{T}_i, \vect{I}\}$ and then generates motion blocks $\{\vect{M}_i^*\}$ recursively, where 
\begin{align}
    \vect{M}_i^* = \mathcal{G}\left(
        \mathcal{E}_M\left(\vect{M}_{i-1}^*\right), 
        \mathcal{E}_A\left(\vect{A}^{\eqword{low}}_{i-1}, \vect{A}^{\eqword{low}}_{i}, \vect{A}^{\eqword{low}}_{i+1}\right), 
        \vect{s}_i^*, \vect{z}_i^*, \vect{P}
        \right),
    \label{eqn:gesture_inference}
\end{align}
and $\mathcal{G}, \mathcal{E}_M, \mathcal{E}_A$ are the components of the learned gesture generator. The generated motion blocks are then denormalized to their original length in the input speech, producing a realistic co-speech gesture animation. Note that we again use the asterisk~($*$) to indicate a computed quantity that is not provided directly in the speech. 

All the variables in \eqn\eqref{eqn:gesture_inference} are known except the gesture lexeme $\vect{s}^*$ and style code $\vect{z}^*$. As shown in \fig\ref{fig:system_overview}, our system learns two interpreters to compute them: the \emph{lexeme interpreter} $\mathcal{P}_s$ translates high-level speech features into the gesture lexemes $\vect{s}^*$, and the \emph{style interpreter} $\mathcal{P}_z$ predicts the style code $\vect{z}^*$ according to the low-level speech features. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig5.pdf}
    \caption{Architecture of the lexeme interpreter.}
    \label{fig:interpreters}
\end{figure}

\subsection{Lexeme Interpreter}
\label{subsec:lexeme_interpreter}
As illustrated in \fig\ref{fig:interpreters}, the lexeme interpreter is formulated as
\begin{equation}
    \vect{s}_i^*=\mathcal{P}_s(\vect{s}_{i-1}^*, \tildevect{A}_i^{\eqword{H}}, \tildevect{T}_i, \vect{I}), 
    \label{eqn:lex_interpreter}
\end{equation}
which is conditioned on the gesture lexeme of the last motion block $\vect{s}_{i-1}^*$, and the high-level features of the current speech block. Like the generator, the high-level audio features $\tildevect{A}_i^{\eqword{H}}\in\mathbb{R}^{K\times{}d_{\tilde{A}}}$ are computed using three consecutive audio blocks
\begin{equation}
    \tildevect{A}_i^{\eqword{H}}=\mathcal{E}_A^{\eqword{lex}}(\vect{A}^{\eqword{high}}_{i-1},\vect{A}^{\eqword{high}}_{i},\vect{A}^{\eqword{high}}_{i+1}),
\end{equation}
where each $\vect{A}^{\eqword{high}}$ contains the high-level representation of the input speech audio, and the encoder $\mathcal{E}_A^{\eqword{H}}$ is a single-layer fully connected network. 
%
The text feature $\tildevect{T}_i\in\mathbb{R}^{K\times{}d_{\tilde{t}}}$ is also extracted from the text representation of the speech block as 
\begin{equation}
    \tildevect{T}_i=\mathcal{E}_T^{\eqword{lex}}(\vect{T}_i),
\end{equation}
where $\mathcal{E}_T^{\eqword{lex}}$ is again a single-layer network. % that changes the dimensionality of the text representation $\vect{T}_i$. 
%
Lastly, the one-hot representation of the speaker ID, $\vect{I}$, is repeated $K$ times and converted into a feature block.

Those feature blocks can then be concatenated together and fed to an LSTM-based decoder to predict the next gesture lexeme $\vect{s}_i^*$. However, considering that the lexemes are selected from the discrete gesture lexicon, we can convert this regression problem into a classification problem. Specifically, instead of directly evaluating \eqn\eqref{eqn:lex_interpreter}, we can let $\mathcal{P}_s$ predict the probability that $\vect{s}_i^*$ is a specific lexeme in the gesture lexicon, then the lexeme with the maximum likelihood will be considered as the result.

\subsection{Style Interpreter}
The style interpreter shares a similar structure with the lexeme interpreter. It computes $\vect{z}_{i}^*$ as
\begin{equation}
    {\vect{z}}_{i}^{*} = \mathcal{P}_z\left(
        \vect{z}_{i-1}^*,
        \vect{s}_{i}^*,
        \mathcal{E}_T^{\eqword{style}}\left(\vect{T}_i\right), 
        \mathcal{E}_A^{\eqword{style}}\left(\vect{A}^{\eqword{low}}_{i-1},\vect{A}^{\eqword{low}}_{i},\vect{A}^{\eqword{low}}_{i+1}\right)
        \right),
    \label{eqn:style_interpreter}
\end{equation}
which is conditioned on the last style code and the new gesture lexeme computed by the lexeme interpreter. The low-level audio representation $\vect{A}^{\eqword{low}}$ is used in the style interpreter.

\subsection{Audio-Only Inference}
\label{subsec:inference_based_on_audio_only}
Both the two interpreters can be reformulated to take only the speech audio as input, where the features related to the text representation $\vect{T}$, and optionally the speaker ID $\vect{I}$, will be removed from \eqn\eqref{eqn:lex_interpreter} and \eqref{eqn:style_interpreter}.

In practice, these audio-only interpreters allow cross-language gesture generation, where the speech audio in another language can be taken as input to synthesize realistic gestures without further training. For example, we can utilize a pre-trained model on an English dataset to generate gestures that accompany a Chinese speech. We will show related experiments in Section \ref{subsec:evaluation}.

\subsection{Training}
During the training of the generator, we have computed the gesture lexeme $\vect{s}_i$ and the style code $\vect{z}_i$ of every motion block in the training dataset. We then train the two interpreters using these results as the ground truth. We minimize the standard categorical cross-entropy loss to train the lexeme interpreter, while the MSE loss is used for the style interpreter.

\subsubsection{Silent Period Hint}
A speaker typically stops gesticulating during a silent pause \cite{graziano2018silence}. Such behaviors are often crucial to the naturalness of a co-speech gesture animation. However, we find that it is often difficult for a gesture generator to deal with silent periods well, even in recent successful systems such as \cite{alexanderson2020style,kucherenko2020gesticulator}. The speech-gesture datasets may lack necessary motion, and some specific generator models, such as LSTM, may exhibit generative inertia that makes it difficult to become stationary in time.

To solve this problem, we develop a new approach, which we refer to as the \emph{silent period hint}, to encourage the lexeme interpreter to compute a specific \emph{silent lexeme} that corresponds to a silent gesture when encountering a silent period. 
We check all the lexemes in the lexicon and label a number of stationary ones as the silent lexemes. Notably, the silent lexemes can be automatically labeled by finding such a lexeme corresponding to an empty text word. Then, when a training audio block is in a silent period, which can be detected by the data module of our system, we will force the lexeme interpreter to output the silent lexeme that is the nearest to the current lexeme in the latent space.
Moreover, a silent data augmentation is applied when training the generator. We find data blocks that contain empty words and randomly insert $0\sim{}10$ consecutive silent blocks after them. The silent block above includes four different features: (a) the audio feature is the environmental noise; (b) the style code is set to zero; (c) the gesture lexeme is the silent lexeme that is the nearest to the previous lexeme in the latent space; and (d) the motion is a stationary pose that is the same as the last frame of the previous motion block. In total, the amount of the inserted silent blocks accounts for $5\%$ of the whole training set.