
%\section{Preparing PostScript or PDF files}
%
%Please prepare PostScript or PDF files with paper size ``US Letter'', and
%not, for example, ``A4''. The -t
%letter option on dvips will produce US Letter files.
%
%Consider directly generating PDF files using \verb+pdflatex+
%(especially if you are a MiKTeX user).
%PDF figures must be substituted for EPS figures, however.
%
%Otherwise, please generate your PostScript and PDF files with the following commands:
%\begin{verbatim}
%	dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
%	ps2pdf mypaper.ps mypaper.pdf
%\end{verbatim}
%
%\subsection{Margins in LaTeX}
%
%Most of the margin problems come from figures positioned by hand using
%\verb+\special+ or other commands. We suggest using the command
%\verb+\includegraphics+
%from the graphicx package. Always specify the figure width as a multiple of
%the line width as in the example below using .eps graphics
%\begin{verbatim}
%	\usepackage[dvips]{graphicx} ...
%	\includegraphics[width=0.8\linewidth]{myfile.eps}
%\end{verbatim}
%or % Apr 2009 addition
%\begin{verbatim}
%	\usepackage[pdftex]{graphicx} ...
%	\includegraphics[width=0.8\linewidth]{myfile.pdf}
%\end{verbatim}
%for .pdf graphics.
%See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})
%
%A number of width problems arise when LaTeX cannot properly hyphenate a
%line. Please give LaTeX hyphenation hints using the \verb+\-+ command.
%


%\subsection{STRENGTHS AND LIMITATIONS}
%
%The proposed DeepGesture model introduces several key advantages that advance the development of more natural and expressive human-machine interaction systems. Nevertheless, certain limitations remain and represent important directions for future work.
%
%\textbf{Strengths:}
%
%\begin{itemize}
%	\item \textbf{High realism:} DeepGesture generates gestures that exhibit strong human-likeness, closely mirroring the timing and rhythm of natural speech. The model effectively synchronizes gestures with both the emotional tone and semantic content of speech.
%	
%	\item \textbf{Robust generalization:} Leveraging the denoising capabilities of diffusion models, DeepGesture can generate gestures for speech and emotional contexts not encountered during training, indicating strong potential for real-world deployment across varied domains.
%	
%	\item \textbf{Multidimensional controllability:} The diffusion-based architecture supports flexible control over multiple attributes, including the interpolation and modulation of emotional states, enabling expressive and context-sensitive gesture synthesis.
%\end{itemize}
%
%\textbf{Limitations:}
%
%\begin{itemize}
%	\item \textbf{Lack of real-time inference:} The current implementation is not optimized for real-time execution and requires multi-step generation followed by offline rendering.
%	
%	\item \textbf{Suboptimal motion representation:} The gesture feature representation with $D=1141$ dimensions is processed as a 2D image structure, which may not fully capture temporal motion dynamics and biomechanical features.
%	
%	\item \textbf{Sensitivity to input quality:} The model’s performance heavily depends on clean, high-quality speech input. In cases of noisy or emotionally ambiguous audio, the gesture output may be less accurate or less expressive.
%\end{itemize}


\section{CONCLUSION}
\label{sec:conclusion}



\subsection{Achieved Results}

This work presents DeepGesture, a gesture generation model that achieves high realism and naturalness through diffusion-based modeling. A primary contribution is its precise synchronization between generated gestures and the emotional content of speech, extending generalization beyond the training distribution. The model demonstrates the ability to produce context-aware gestures even in low-probability speech scenarios.

Another major advancement lies in expanding input modalities. In addition to speech, gesture, and emotion labels, the model incorporates textual features obtained through automatic transcription. This multimodal approach enables the system to capture the semantic context of input speech more effectively and generate more appropriate gestures.

\subsection{Future Work}

Several promising directions can further improve DeepGesture and expand its applicability in real-world settings:

\begin{itemize}
	\item \textbf{Real-time inference optimization:} Future efforts will focus on transforming DeepGesture into a real-time gesture generation system by reducing dependencies on offline rendering tools (e.g., Unity) and optimizing latency for interactive applications.
	
	\item \textbf{Efficient sampling:} The current diffusion process involves many sampling steps. Reducing these without compromising generation quality remains an important area for improving system responsiveness.
	
	\item \textbf{Advanced embeddings:} Incorporating richer embedding techniques-potentially combining speech, text, prosody, and affective signals-could further enhance gesture appropriateness across different languages and cultural contexts.
	
	\item \textbf{Multilingual generalization:} Extending the model to support multiple languages will broaden its applicability and ensure culturally appropriate gesture behavior.
	
	\item \textbf{Integration with phase-aware models:} A future integration with the DeepPhase model~\citep{starke2022deepphase} is planned to improve the representation of temporal dynamics and support real-time applications via phase-informed motion generation.
	
	\item \textbf{Improved automatic metrics:} To reduce reliance on subjective human evaluation, ongoing work aims to design robust automatic evaluation metrics that better reflect human perception and can serve as internal feedback mechanisms during training and inference.
\end{itemize}

\subsection{Closing Remarks}

Through experimental validation and qualitative analysis, the DeepGesture model -- extending DiffuseStyleGesture-demonstrates the ability to generate realistic gestures for both in -- distribution and out-of-distribution speech, including synthetic voices such as that of Steve Jobs (see \autoref{appendix3}). This highlights the promise of diffusion-based approaches in modeling complex, expressive, and low-frequency gesture behaviors.

Additionally, we contribute open-source code, including rendering pipelines and data processing tools built on Unity, available on GitHub. These resources provide a solid foundation for future development and reproducibility. The integration of text alongside speech and emotion into the gesture generation pipeline marks an important step toward building fully multimodal agents, capable of more intuitive and human-like interactions in diverse application domains.

\newpage
\newpage

%\subsubsection*{Author Contributions}
%If you'd like to, you may include  a section for author contributions as is done
%in many journals. This is optional and at the discretion of the authors.
%
%\subsubsection*{Acknowledgments}
%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments, including those to funding agencies, go at the end of the paper.


%\subsection{Thesis Contributions}
%\label{sec:contribution}
%
%In this study, the DeepGesture gesture generation system was developed, with the following key contributions:
%
%\begin{itemize}
%	\item \textbf{Development of a gesture generation model based on Diffusion:} The DeepGesture system is designed to generate gestures synchronized with input speech and accurately reflecting emotion. The model also possesses generalization capabilities, allowing gesture generation even for speech samples outside the training data, thereby achieving high realism.
%	
%	\item \textbf{Open-sourcing code and models on public platforms:} To encourage community adoption and improvement, we provides source code on GitHub, with extensions and releases available at \hyperlink{https://github.com/hmthanh/DeepGesture}{Github/DeepGesture} \footnote{GitHub source code: \url{https://github.com/hmthanh/DeepGesture}} and a pretrained version on Huggingface at \hyperlink{https://huggingface.co/openhuman/openhuman}{huggingface.co/openhuman/openhuman} \footnote{HuggingFace: \url{https://huggingface.co/openhuman/openhuman}}, enabling other researchers to easily access, reproduce, and extend the system.
%	
%	\item \textbf{Integration of text and transcribed speech in gesture generation:} Since the ZeroEGGS dataset includes only speech, gesture, and emotion labels, this paper uses Azure and Google APIs to transcribe the speech files into text. This enriches the model’s input with textual features, giving the system additional context to produce more semantically appropriate gestures.
%	
%	\item \textbf{Contribution to standardized evaluation systems:} We developed an online ranking system \hyperlink{https://genea-workshop.github.io/leaderboard/}{GENEA Leaderboard} \footnote{GENEA Leaderboard: \url{https://genea-workshop.github.io/leaderboard/}} \citep{nagy2024towards} for gesture generation models. The GENEA Leaderboard collects and processes gesture data from multiple languages and datasets into a unified benchmark, allowing comparative evaluation across various models. Human evaluators are used to assess the models, providing more accurate evaluations of gesture generation results compared to previous metrics, which fail to capture the complexity and diversity of speech-related motion. This creates a unified data foundation that promotes consistent evaluation within the gesture generation research community.
%	
%	\item \textbf{Development of a Unity-based visualization tool:} Existing gesture visualization systems rely on Blender and do not render gestures effectively. By extending the source code of the DeepPhase model \citep{starke2022deepphase}, we developed a Unity-based rendering system \hyperlink{https://github.com/DeepGesture/deepgesture-unity}{Github/DeepGesture-Unity} \footnote{Unity-based gesture generation rendering system: \url{https://github.com/DeepGesture/deepgesture-unity}}.
%	
%	\item \textbf{Development of gesture evaluation using FGD (Fréchet Gesture Distance):} Based on the FGD source code \citep{yoon2020speech}, this paper builds GestureScore and trains a new model to evaluate distribution differences in joint rotation angles between predicted and ground-truth data. The code is available at \hyperlink{https://github.com/GestureScore/GestureScore}{GestureScore} \footnote{Github/GestureScore: \url{https://github.com/GestureScore/GestureScore}} and the pretrained evaluation model is available on \hyperlink{https://huggingface.co/GestureScore}{Huggingface} \footnote{Huggingface/GestureScore: \url{https://huggingface.co/GestureScore/GestureScore}}.
%	
%	\item \textbf{Outlining future development directions:} Based on the diffusion model and a deep understanding of the gesture generation process, we proposes integrating phase-based gesture generation models with advanced processing and feature extraction algorithms to optimize the quality and contextual alignment of generated gestures. This development direction opens up opportunities for significant improvements in gesture interaction with complex contextual elements such as facial expressions, prosody, and emotional dynamics, laying the groundwork for advancements in human-machine communication and related fields.
%\end{itemize}
%
